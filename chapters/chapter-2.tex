\chapter{Matrix Exponential Methods}\label{ch:matrix_exp_methods}
Many methods exist for solving systems of first order differential equations. In vector matrix form, the problem involves solving a system of ordinary differential equations of the form,

\begin{equation}
    \frac{d\boldsymbol{y}}{dt} = \boldsymbol{f}(t,\boldsymbol{y}), \quad \boldsymbol{y}({t_{0})} = \boldsymbol{y_{0}}
    \label{eq:ODE_matrix_form}
\end{equation}{}

\noindent
where $\boldsymbol{y}$, $\boldsymbol{f}(t,\boldsymbol{y})$ and $\boldsymbol{y}_{0}$ are column vectors and $\boldsymbol{f}(t,\boldsymbol{t})$ can contain linear and nonlinear terms. 

$$
\boldsymbol{y}(t) = 
\begin{bmatrix}
y_{1}(t) \\
y_{2}(t) \\
\vdots \\
y_{m}(t) \\
\end{bmatrix}, \quad 
\boldsymbol{f}(t,\boldsymbol{y}) = 
\begin{bmatrix}
f_{1}(t, y_{1}, y_{2}, \dots y_{m}) \\ 
f_{2}(t, y_{1}, y_{2}, \dots y_{m}) \\ 
\vdots \\
f_{m}(t, y_{1}, y_{2}, \dots y_{m}) \\ 
\end{bmatrix}, \quad
\boldsymbol{y}_{0} = 
\begin{bmatrix}
y_{1,0} \\
y_{2,0} \\
\vdots \\ 
y_{m,0} \\
\end{bmatrix}
$$

There are many popular methods to solve these equations including Euler, Runge–Kutta and multistep. These methods involve dividing the time domain into discrete lengths and stepping through the domain by approximating each time step from the previous steps solution and a slope between the points. When the function vector $\boldsymbol{f}(t,\boldsymbol{y})$ contains terms that are stiff, these methods become computationally expensive and inaccurate for larger time steps \cite{ash2009} \cite{ODECh82011}.

A more adventitious method for solving Equation \ref{eq:ODE_matrix_form} is the use of exponential time differencing  \cite{ash2009} \cite{cox2002} \cite{bratsos2019}. To explain this method, Equation \ref{eq:ODE_matrix_form} is rewritten in a form that decomposes $\boldsymbol{f}(t,\boldsymbol{y})$ into two operators, one representing a constant linear operator and one for the nonlinear operator. Equation \ref{eq:ODE_matrix_linear_nonlinear_form} shows this separation with $\boldsymbol{L}$ being the linear operator and $\boldsymbol{N}$ being the nonlinear.


\begin{equation}
    \frac{d\boldsymbol{y}}{dt} = \boldsymbol{Ly} + \boldsymbol{N}(t,\boldsymbol{y}) 
    \label{eq:ODE_matrix_linear_nonlinear_form}
\end{equation}{}

Solving Equation \ref{eq:ODE_matrix_linear_nonlinear_form} using a class of methods involving matrix exponentials can  be broken down into two main categories, integration factor and exponential time differencing. Both of the previously stated methods are discussed further in the next sections. 

\section{Integrating Factor Method}
The integrating factor method begins by defining the following expression \cite{Kassam2005},

\begin{equation}
    \boldsymbol{v} = e^{-\boldsymbol{L}t}\boldsymbol{y},
    \label{eq:v_deff_IF_method}
\end{equation}{}

\noindent
where $e^{-\boldsymbol{L}t}$ is the integrating factor. Next, Equation \ref{eq:v_deff_IF_method} is differentiated with respect with time to give,

\begin{equation}
    \frac{d\boldsymbol{v}}{dt} = -e^{-\boldsymbol{L}t}\boldsymbol{L}\boldsymbol{y} + e^{-\boldsymbol{L}t} \frac{d\boldsymbol{y}}{dt}.
\end{equation}{}

\noindent
Multiplying Equation \ref{eq:ODE_matrix_linear_nonlinear_form} by the integrating factor and bringing the linear operator to the right hand side gives, 

\begin{equation}
    e^{-\boldsymbol{L}t}\frac{d\boldsymbol{y}}{dt} - e^{-\boldsymbol{L}t}\boldsymbol{L}\boldsymbol{y} = e^{-\boldsymbol{L}t}\boldsymbol{N}(t,\boldsymbol{y}) = \frac{d\boldsymbol{v}}{dt}.
\end{equation}{}

\noindent
This brings the final form of the equation to solve, 

\begin{equation}
    \frac{d\boldsymbol{v}}{dt} = e^{-\boldsymbol{L}t}\boldsymbol{N}(t,e^{\boldsymbol{L}t}\boldsymbol{v}), 
\end{equation}{}

\noindent
or, 

\begin{equation}
    \frac{d\boldsymbol{v}}{dt} = \boldsymbol{f}(t,\boldsymbol{v}).
    \label{eq:IFM_transformed_equation_to_solve}
\end{equation}{}

Solving Equation \ref{eq:IFM_transformed_equation_to_solve} can be done using any usual Runge–Kutta, or multistep method. For example, lets solve using a fourth order Runge-Kutta method \cite{Kassam2005}.


\begin{equation*}
    k_{1} = h\boldsymbol{f}(t_{n},\boldsymbol{v}_{n})
\end{equation*}{}
\vspace*{-1.0cm}
\begin{equation*}
    k_{2} = h\boldsymbol{f}(t_{n}+h/2, \boldsymbol{v}_{n} + k_{1}/2)
\end{equation*}{}
\vspace*{-1.0cm}
\begin{equation*}
    k_{3} = h\boldsymbol{f}(t_{n}+h/2, \boldsymbol{v}_{n} + k_{2}/2)
\end{equation*}{}
\vspace*{-1.0cm}
\begin{equation*}
    k_{4} = h\boldsymbol{f}(t_{n}+h, \boldsymbol{v}_{n} + k_{3})
\end{equation*}{}
\vspace*{-1.0cm}
\begin{equation}
    \boldsymbol{v}_{n+1} \approx \boldsymbol{v}_{n} + \frac{1}{6}(k_{1} + 2k_{2} + 2k_{3} + k_{4})
\end{equation}{}

\noindent



\noindent
Integrating factor methods have the property of being exact when $\boldsymbol{N}(t,\boldsymbol{y}) = 0$ \cite{ash2009}. 

\section{Exponential Time Differencing}
Exponential time differencing methods are very similar to the integrating factor method except for how it handles the nonlinear portion of Equation \ref{eq:ODE_matrix_linear_nonlinear_form}. First taking the integrating factor and multiply it through Equation \ref{eq:ODE_matrix_linear_nonlinear_form} and integrating over the time step \cite{cox2002}. 

\begin{equation}
    \boldsymbol{y}_{n+1} = \boldsymbol{y}_{n}e^{\boldsymbol{L}\Delta t} + e^{\boldsymbol{L}\Delta t} \int_{0}^{\Delta t} e^{-\boldsymbol{L}\tau}N(\boldsymbol{y}(t_{n} + \tau), t_{n}+\tau)d\tau
\end{equation}{}

\noindent
This formalization is exact and exponential time differencing methods work to approximate the integral of the nonlinear portion. 

\section{Solutions to the Matrix Exponential}
When obtaining solutions based on exponential time differencing or integrating factor methods, an exponential of a matrix needs to be computed. There are multiple computational methods for solving for the matrix exponential, many of them are developed specifically to evaluate $e^{\boldsymbol{A}}$ or $e^{\boldsymbol{A}}\boldsymbol{v}$. One evaluates the matrix exponential directly and the other calculates the action of the exponential on a vector. When $\boldsymbol{A}$ is dense or has a low to moderate size (few hundreds) methods such as Pad\'e approximation or Chebyshev rational approximation can be employed. For large sparse matrices, Kraylov subspace methods offer more efficient calculations \cite{exokit}. In situations when the matrix $\boldsymbol{A}$ is diagonal, computing the matrix exponential becomes,

$$
e^{\boldsymbol{A}} = 
\begin{bmatrix}
a_{1,1} & 0 & 0 & \hdots & 0 \\
0 & a_{2,2} & 0 & \hdots & 0 \\
0 & 0 & a_{3,3} & \hdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \hdots & a_{m,m} \\
\end{bmatrix}
=
\begin{bmatrix}
e^{a_{1,1}} & 0 & 0 & \hdots & 0 \\
0 & e^{a_{2,2}} & 0 & \hdots & 0 \\
0 & 0 & e^{a_{3,3}} & \hdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \hdots & e^{a_{m,m}} \\
\end{bmatrix}
.
$$

\noindent
These sort of situations arise when solving PDEs using spectral methods \cite{ash2009} \cite{cox2002} \cite{kazimi1990}. In this chapter multiple methods for evaluating the matrix exponential are presented. Each of these methods come with various advantages and disadvantages and specific matrix properties that are required for computation. 



\subsection{Taylor Series Expansion}
Formally,  matrix exponential is defined using an infinite Taylor series \cite{exokit} \cite{moler2003} \cite{pusa2010}. 
\begin{equation}
    e^{\boldsymbol{A}} = \sum_{k = 0}^{\infty}\frac{1}{k!}\boldsymbol{A}^{k}
    \label{eq:power_series_exp}
\end{equation}

\noindent This method however, is not commonly used in application for either the matrix or the scalar case. The number of terms required to achieve convergence can be large and produce computational inefficiency. This method is also suffers from numerical round off errors from cancellation for large values of $k$ \cite{moler2003}. 


\subsection{Pad\'e Approximation}
The Pad\'e approximation represents a function by expanding it as a ratio of two power series. A ($p,q$) Pad\'e approximation for $e^{\boldsymbol{A}}$ is defined by \cite{moler2003}, 

\begin{equation}
    R_{p,q} = \frac{N_{p,q}(\boldsymbol{A})}{D_{p,q}(\boldsymbol{A})}
    \label{eq:padeApprox}
\end{equation}

\noindent where

\begin{equation*}
    N_{p,q}(\boldsymbol{A}) = \sum_{j=0}^{P}\frac{(p + q - j)!p!}{(p + q)!j!(p - j)!}\boldsymbol{A}^{j}
\end{equation*}

\begin{equation*}
    D_{p,q}(\boldsymbol{A}) = \sum_{j=0}^{P}\frac{(p + q - j)!q!}{(p + q)!j!(q - j)!}(-\boldsymbol{A})^{j}.
\end{equation*}

\noindent The error associated with the previous Pad\'e approximation is demonstrated by \cite{higham2005}

\begin{equation}
    e^{\boldsymbol{A}} - R_{p,q}(\boldsymbol{A}) = (-1)^{m}\frac{k!m!}{(k+m)!(k+m+1)!}\boldsymbol{A}^{k+m+1} + \mathcal{O}(\boldsymbol{A}^{k+m+2}).
\end{equation}

Pad\'e methods are similar to Taylor series as they approximate a function using a series solution, however, Pad\'e series usually out preform Taylor series. Series solutions methods, such as Pad\'e are also more accurate near the origin, meaning that the matrix norm $||A||$ must be sufficiently small for the approximation to be accurate \cite{pusa2010}. Yet another problem arises when $\boldsymbol{A}$ has a wide spread of eigenvalues, causing an ill-conditioned linear system  \cite{exokit} \cite{moler2003}.  

When applying the Pad\'e approximation it is often adventitious to set $p$ and $q$ equal to one another, this is referred to as diagonal approximations. Setting $q = p$ requires about the same amount of work if $q > p$ for the same $q$, but would result in a approximation that is of order $2p > p + q$. Another reason to set $q=p$ comes from examining the spectrum of $\boldsymbol{A}$. If the eigenvalues of $\boldsymbol{A}$ are located on the left hand side of the complex plane, then $e^{\boldsymbol{A}t}$ goes to zero as $t$ goes to infinity. With $q > p$, $R_{pq}(\boldsymbol{A}t)$ is bounded, but if $q < p$, $R_{pq}(\boldsymbol{A}t)$ is unbounded \cite{moler2003}. 


Instead of directing using Equation \ref{eq:padeApprox}, Horners rule can be applied to the numerator and denominator to reduce the number of operations. This result is shown in Equation \ref{eq:hornerPadeApprox}. 

\begin{equation}
    R_{pp}(\boldsymbol{A})=
    \begin{cases}
        1+2\frac{\boldsymbol{A}\sum_{k=0}^{p/2-1}c_{2k+1}\boldsymbol{A}^{2k}}{\sum_{k=0}^{p/2}c_{2k}\boldsymbol{A}^{2k}-\boldsymbol{A}\sum_{k=0}^{p/2-1}c_{2k+1}\boldsymbol{A}^{2k}} & p \text{ even}\\[1em]
        
        -1-2\frac{\sum_{k=0}^{(p-1)/2}c_{2k}\boldsymbol{A}^{2k}}{\boldsymbol{A}\sum_{k=0}^{(p-1)/2}c_{2k+1}\boldsymbol{A}^{2k}-\sum_{k=0}^{(p-1)/2}c_{2k}\boldsymbol{A}^{2k}} & p \text{ odd}
    \end{cases}
    \label{eq:hornerPadeApprox}
\end{equation}

\noindent where

\begin{equation*}
    c_{k} = c_{k-1}\frac{p+1-k}{(2p+1-k)k}
\end{equation*}

\noindent with $c_{0} = 1$. This evaluates Equation \ref{eq:padeApprox} with half the number of operations \cite{exokit}.


\subsection{Scaling and Squaring}
Both Taylor and Pad\'e methods fail to approximate the matrix exponential when $||\boldsymbol{A}||$ or the spread of the eigenvalues are large. This problem is exacerbated when computing $e^{\boldsymbol{A}t}$ for sufficiently large values of $t$. The scaling and squaring method works to fixed these problem by exploiting the property

\begin{equation*}
    e^{\boldsymbol{A}} = \big( e^{\boldsymbol{A}/m}\big)^{m}
    \label{eq:scalingAndSquaring},
\end{equation*}

\noindent $m$ is a scalar chosen to be $2^{\alpha}$ where $\alpha$ is the number of times the matrix is squared \cite{moler2003}. 

Moler and Van Loan \cite{moler2003} derived an error analysis for choosing the proper value of $m$ based on $||\boldsymbol{A}||$. As noted by Higham \cite{higham2005}, this derivation contained weaknesses. Moler assumed that the matrix norm needed to be less than one half ($||A|| < 1/2$) but Higham proved that this was not the case. Higham further showed that the required minimal matrix norm is different for each order of the Pad\'e implementation. Above this norm, a higher order Pad\'e approximation would be required, or matrix scaling would need to be take place. One other weakness was the derivation of their error bound. It was designed to be easily computable, which resulted in their error bound not being sharp \cite{higham2005}. 

%\begin{table}[t]
%   \caption{\label{tab:scalingSquaringOptimalValues} Summary of optimal Pad\'e orders and required matrix multiplications \cite{higham2005}}
%   \centering
%   \begin{tabular}{c| cccccccccc}
%   \hline
%   $m$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\
%   \hline 
%    $\theta_{m}$ & 3.7e-8 & 5.3e-4 & 1.5e-2 & 8.5e-2 & 2.5e-1 & 5.4e-1 & 9.5e-1 & 1.5e0 & 2.1e0 & 2.8e0 \\
%   \hline
%   $\pi_{m}$ & 0 & 1 & 2 & 3 & 3 & 4 & 4 & 5 & 5 & 6 \\
%   \hline
%   \end{tabular}

%   \vspace*{4 mm}
   
%   \begin{tabular}{c| ccccccccccc}
%   \hline
%   $m$ & 11 & 12 & 13 & 14 & 15 & 16 & 17 & 18 & 19 & 20 & 21\\
%   \hline 
%    $\theta_{m}$ & 3.6e0 & 4.5e0 & 5.4e0 & 6.3e0 & 7.3e0 & 8.4e0 & 9.4e0 & 1.1e1 & 1.2e1 & 1.3e1 & 1.4e1 \\
%   \hline
 %  $\pi_{m}$ & 6 & 6 & 6 & 7 & 7 & 7 & 7 & 8 & 8 & 8 & 8 \\
 %  \hline
 %  \end{tabular}
%\end{table}





\subsection{Solutions Based on the Cauchy Integral Formula}


